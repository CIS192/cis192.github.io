{"componentChunkName":"component---src-templates-blog-post-js","path":"/assignments/2","result":{"data":{"markdownRemark":{"rawMarkdownBody":"\n# Assignment 2: Learning Machine Learning\n\nIn this assignment, we will be exploring data science, and building our first machine learning model from scratch! Make sure to install dependencies with `pip install numpy pandas` as we'll be using them for the assignment. Try to explore other dependency managers such as [Pipenv](https://pipenv.pypa.io/en/latest/) and [Poetry](https://python-poetry.org/) as well! In addition, make sure to keep your data files in the same directory as your `assignment2.py` ([available here](https://raw.githubusercontent.com/CIS192/homework/master/assignment2/assignment2.py)) to keep things simple.\n\n## Part 1: Data Wrangling with Pandas\n\n### Section 0: Pandas Pandas Pandas Pandas (x4)\n\nOne of Python's biggest assets is its ability to let us do data analysis quickly and effortlessly, in a reproducible way. Since data analysis is a prevalent part of any field in both academia, and industry, it's worth having good data skills in your toolbelt.\n\nThe best way to do quick data analysis in Python is to use [Pandas](https://pandas.pydata.org), an open source data analysis library used by virtually every data scientist. In particular, we will be looking at a dataset of [different types of wines](https://archive.ics.uci.edu/ml/datasets/wine) and their chemical makeup. Cheers!\n\nRecall that you can access a column from a DataFrame with a dictionary-like notation:\n\n```python\ncolumns = df['column']\nquery = df[df['column'] == True]\n# >>> returns a dataframe where the inner query is satisfied\n```\n\n### Section 1: Scavenger Hunt\n\nIn this scavenger hunt, you will be using Pandas to determine the answers to the following questions about the provided [Wine Dataset](https://raw.githubusercontent.com/CIS192/homework/master/assignment2/wine.csv):\n\n0. What are the dimensions of the dataframe? That is, how many rows and columns are there?\nGive your answer as a `(rows, columns)` tuple.\n\n1. What is the average `alcohol` content over all wines?\n\n2. What is the standard deviation of the `magnesium` content?\n\n3. What is the mean alcohol content, grouped by `target`?\nGive your answer in terms of a list (e.g. `[class_0_alc, class_1_alc, class_2_alc]`).\n\n4. What is the minimum `proline` content? What is the maximum? Give your answer\nas a list in the form `[min, max]`.\n\n5. How many wine samples in the dataset have a `malic_acid` content over 2.5?\n\n6. What is the index of the wine with the lowest `flavanoid` content?\n\n7. How many unique `hue` values are there in the dataset?\n\n**TODO:** Implement the `scavenger_hunt()` function by returning a dictionary mapping the question number (as an integer) to the correct answers.\n\n*Hint*: You can read the file into a `DataFrame` by using:\n\n```python\ndf = pd.read_csv('wine.csv')\n```\n\nMethods that might be useful are: `mean()`, `std()`, `size()`, `groupby()`, `sort_values()`, `idxmin()`, and `unique()`. Feel free to use outside resources to figure out creative ways of answering these questions. The [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/) will definitely be helpful.\n\n## Part 2: Generating Text With N-Gram Language Models\n\n### Section 0: What is a language model?\n\nIn this assignment, we will be building a **language model**, which is how we can machine learning to generate text (e.g. chatbots, summarization, translation). In particular, we will be training an **n-gram** model, which is a relatively simple but extremely powerful model. Next week, we'll work with the state-of-the-art in NLP, which includes neural networks!\n\nIn short, an n-gram language model works by treating language as a sequence of *overlapping* word tuples, of size $n$. For example, the sentence \"I love CIS 192\" would be represented as unigrams ($n = 1$) as:\n\n```python\n[(\"I\"), (\"love\"), (\"CIS\"), (\"192\")]\n```\n\nand as bigrams ($n = 2$) as:\n\n```python\n[(\"I\", \"love\"), (\"love\", \"CIS\"), (\"CIS\", \"192\")]\n```\n\nLanguage models work by estimating the **probability of the next word** in sequence given the previous words, and sampling from that distribution. If you're familiar with the **Markov Property**, we will be relying on it as a powerful assumption to make the generation process simpler. Powerful language models (such as [GPT-3](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/) and the human brain) should include all the history in a sentence. However, for this assignment, we will just be considering a single n-gram of history to make the math simpler.\n\nAt a high level, in this assignment, we will build an estimator for the probability of a given word (or n-gram), given a context (n-gram), and then a way to repeatedly select new words until we have a full body of text.\n\nWe've provided some stubs to make the implementation process more clear. Feel free to add helper functions, but as usual, don't change the type signatures of the functions! You can get an idea of how the functions should work together by checking out the code under `if __name__ == \"__main__\"`.\n\n### Section 1: Reading Data\n\nThe first thing you want to do is load the data from a given text file into memory. We'll start by just getting a list of the words, and chunking them into grams later. Use the `corpus.txt` file ([available here](https://raw.githubusercontent.com/CIS192/homework/master/assignment2/corpus.txt)) as a example corpus to use.\n\n**TODO:** Implement the `get_words()` function, that takes in the file path of a plain-text `.txt` as a string. The function should return a list of lowercase words (as strings), in the order that they appear in the text file.\n\n*Hint:* make use of the `split()` function and `lower()`.\n\n### Section 2: Transforming Data\n\nGetting data into a useable format is often most important part of the machine learning process. For an n-gram model, this means taking our list of words and creating our list of n-grams, provided the value for $n$.\n\n**TODO:** Implement `get_ngrams()`, which takes a list of words and the size of the grams and returns a **list of tuples**, where each tuple is a gram.\n\n### Section 3: Computing the Distribution of Words\n\nThe most important part of the n-gram model is our estimate of the distribution of our words. That is, we want to map a particular context n-gram to possible next n-grams and their frequencies. We'll represent this distribution by using a double dictionary, where the key of the outer dictionary is the context n-gram, and the inner dictionary maps the target n-gram to its frequency.\n\nFor example, many our `counts` dictionary looks something like:\n\n```python\ncounts = get_counts(n_grams)\nprint(counts[('I', 'am')])\n\n>>> {('am', 'cool'): 50, ('am', 'lame'): 20, ('am', 'asleep'): 10 ... }\n```\n\nWe also want to make sure that our model *generalizes* a bit better than just the raw frequencies. So we'll also add 1 to each possible frequency so that our model has a bit more creativity. If you're interested, this is called [smoothing](https://en.wikipedia.org/wiki/Language_model#n-gram).\n\n*Hint:* We can do this by initializing a default dictionary, which maps to a default dictionary with the default integer value of 1: \n\n```python\ncounts = defaultdict(lambda: defaultdict(lambda: 1))\n```\n\n**TODO:** Implement the `get_counts()` function, which takes in the list of n-grams and returns the frequency distribution of the n-grams.\n\n*Hint:* The `defaultdict` with a bunch of `lambda` functions may be hard to interpret when `print`-ing. I would recommended casting the `defaultdict` to a regular `dict` to make this cleaner and easier to read.\n\n### Section 4: Generating Words By Sampling\n\nNow, it's time for the moment of truth: using our distribution of frequencies to select the next word given some context. \n\nOne way to randomly select a next word would be to select a random number between 1 and `len(n_grams)`, and select that n-gram. However, that totally disregards our actual empirical estimate of the frequencies (i.e. in `get_counts`). A better way would be to randomly select a word in accordance with the distribution we computed earlier.\n\nWe can do this with [NumPy](https://numpy.org/) by using `np.random.choice`, where we provide the length of a list to sample from and a list of corresponding **probabilities**, which will return the index of the selection:\n\n```python\nword = words[np.random.choice(len(words), p=probabilities)]\n```\n\n**TODO:** Implement the `generate_gram()` function, which takes in the distribution `counts` as well as the context n-gram, and returns a selected n-gram tuple according to the distribution for the given context.\n\n*Hint:* Make sure you understand which dictionary in `counts` to use as the distribution. You also will need to normalize the raw frequencies into probabilities for `np.random.choice` by dividing each entry by the sum of the frequencies in the distribution.\n\nThere's a lot more nuance to generating text than just random sampling, but these suffices for now. If you want to learn more about how to generate text, check out [this blog post I wrote](https://arun.ai/blog/decoding) on the subject.\n\n### Section 5: Generating Entire Sentences\n\nFor the final part, we will put everything together. In order to generate an entire sentence or paragraph, we can repeatedly call `generate_gram()`, feeding in the previous prediction as the next context. This is known as **auto-regressive** generation.\n\n**TODO:** Implement the `generate_sentence` function, which takes in the distribution `counts` as well as a context n-gram (and an optional parameter for the number of n-grams to generate) and should return a list of tuples.\n\n*Hint:* We've provided a `stringify()` helper function to help you visualize what your generated sentence looks like in regular string format. Feel free to use it to help appreciate your work!\n\n### Section 6: Build Your Own Corpus\n\nThe corpus provided consists of presidential speeches from Barack Obama, which can be a little dry. In this section, create your own corpus of text, and be creative with it! It can be things like Wikipedia, blog posts, tweets, fan-fiction, or your friend's literary assignments!\n\nInclude the corpus along with a sample generation in your Gradescope submission.\n\n## Conclusion\n\nMachine learning is a very interesting, but often confusing subject. In this course, unfortunately, we can't provide all the details and depth, but this assignment should serve as a springboard for all sorts of things related to data science. The final project will be a great opportunity to explore some of these ideas in greater depth.\n\nIn the next few assignments and lectures, we'll be introducing modern approaches to this very problem of text generation, such as word embeddings and neural networks!","html":"<h1>Assignment 2: Learning Machine Learning</h1>\n<p>In this assignment, we will be exploring data science, and building our first machine learning model from scratch! Make sure to install dependencies with <code class=\"language-text\">pip install numpy pandas</code> as we'll be using them for the assignment. Try to explore other dependency managers such as <a href=\"https://pipenv.pypa.io/en/latest/\">Pipenv</a> and <a href=\"https://python-poetry.org/\">Poetry</a> as well! In addition, make sure to keep your data files in the same directory as your <code class=\"language-text\">assignment2.py</code> (<a href=\"https://raw.githubusercontent.com/CIS192/homework/master/assignment2/assignment2.py\">available here</a>) to keep things simple.</p>\n<h2>Part 1: Data Wrangling with Pandas</h2>\n<h3>Section 0: Pandas Pandas Pandas Pandas (x4)</h3>\n<p>One of Python's biggest assets is its ability to let us do data analysis quickly and effortlessly, in a reproducible way. Since data analysis is a prevalent part of any field in both academia, and industry, it's worth having good data skills in your toolbelt.</p>\n<p>The best way to do quick data analysis in Python is to use <a href=\"https://pandas.pydata.org\">Pandas</a>, an open source data analysis library used by virtually every data scientist. In particular, we will be looking at a dataset of <a href=\"https://archive.ics.uci.edu/ml/datasets/wine\">different types of wines</a> and their chemical makeup. Cheers!</p>\n<p>Recall that you can access a column from a DataFrame with a dictionary-like notation:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">columns <span class=\"token operator\">=</span> df<span class=\"token punctuation\">[</span><span class=\"token string\">'column'</span><span class=\"token punctuation\">]</span>\nquery <span class=\"token operator\">=</span> df<span class=\"token punctuation\">[</span>df<span class=\"token punctuation\">[</span><span class=\"token string\">'column'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># >>> returns a dataframe where the inner query is satisfied</span></code></pre></div>\n<h3>Section 1: Scavenger Hunt</h3>\n<p>In this scavenger hunt, you will be using Pandas to determine the answers to the following questions about the provided <a href=\"https://raw.githubusercontent.com/CIS192/homework/master/assignment2/wine.csv\">Wine Dataset</a>:</p>\n<ol start=\"0\">\n<li>What are the dimensions of the dataframe? That is, how many rows and columns are there?\nGive your answer as a <code class=\"language-text\">(rows, columns)</code> tuple.</li>\n<li>What is the average <code class=\"language-text\">alcohol</code> content over all wines?</li>\n<li>What is the standard deviation of the <code class=\"language-text\">magnesium</code> content?</li>\n<li>What is the mean alcohol content, grouped by <code class=\"language-text\">target</code>?\nGive your answer in terms of a list (e.g. <code class=\"language-text\">[class_0_alc, class_1_alc, class_2_alc]</code>).</li>\n<li>What is the minimum <code class=\"language-text\">proline</code> content? What is the maximum? Give your answer\nas a list in the form <code class=\"language-text\">[min, max]</code>.</li>\n<li>How many wine samples in the dataset have a <code class=\"language-text\">malic_acid</code> content over 2.5?</li>\n<li>What is the index of the wine with the lowest <code class=\"language-text\">flavanoid</code> content?</li>\n<li>How many unique <code class=\"language-text\">hue</code> values are there in the dataset?</li>\n</ol>\n<p><strong>TODO:</strong> Implement the <code class=\"language-text\">scavenger_hunt()</code> function by returning a dictionary mapping the question number (as an integer) to the correct answers.</p>\n<p><em>Hint</em>: You can read the file into a <code class=\"language-text\">DataFrame</code> by using:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span><span class=\"token string\">'wine.csv'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Methods that might be useful are: <code class=\"language-text\">mean()</code>, <code class=\"language-text\">std()</code>, <code class=\"language-text\">size()</code>, <code class=\"language-text\">groupby()</code>, <code class=\"language-text\">sort_values()</code>, <code class=\"language-text\">idxmin()</code>, and <code class=\"language-text\">unique()</code>. Feel free to use outside resources to figure out creative ways of answering these questions. The <a href=\"https://pandas.pydata.org/pandas-docs/stable/\">Pandas Documentation</a> will definitely be helpful.</p>\n<h2>Part 2: Generating Text With N-Gram Language Models</h2>\n<h3>Section 0: What is a language model?</h3>\n<p>In this assignment, we will be building a <strong>language model</strong>, which is how we can machine learning to generate text (e.g. chatbots, summarization, translation). In particular, we will be training an <strong>n-gram</strong> model, which is a relatively simple but extremely powerful model. Next week, we'll work with the state-of-the-art in NLP, which includes neural networks!</p>\n<p>In short, an n-gram language model works by treating language as a sequence of <em>overlapping</em> word tuples, of size <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span>. For example, the sentence \"I love CIS 192\" would be represented as unigrams (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">n = 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span>) as:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"I\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"love\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"CIS\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"192\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>and as bigrams (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">n = 2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span></span></span></span>) as:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"I\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"love\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"love\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"CIS\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"CIS\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"192\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>Language models work by estimating the <strong>probability of the next word</strong> in sequence given the previous words, and sampling from that distribution. If you're familiar with the <strong>Markov Property</strong>, we will be relying on it as a powerful assumption to make the generation process simpler. Powerful language models (such as <a href=\"https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/\">GPT-3</a> and the human brain) should include all the history in a sentence. However, for this assignment, we will just be considering a single n-gram of history to make the math simpler.</p>\n<p>At a high level, in this assignment, we will build an estimator for the probability of a given word (or n-gram), given a context (n-gram), and then a way to repeatedly select new words until we have a full body of text.</p>\n<p>We've provided some stubs to make the implementation process more clear. Feel free to add helper functions, but as usual, don't change the type signatures of the functions! You can get an idea of how the functions should work together by checking out the code under <code class=\"language-text\">if __name__ == &quot;__main__&quot;</code>.</p>\n<h3>Section 1: Reading Data</h3>\n<p>The first thing you want to do is load the data from a given text file into memory. We'll start by just getting a list of the words, and chunking them into grams later. Use the <code class=\"language-text\">corpus.txt</code> file (<a href=\"https://raw.githubusercontent.com/CIS192/homework/master/assignment2/corpus.txt\">available here</a>) as a example corpus to use.</p>\n<p><strong>TODO:</strong> Implement the <code class=\"language-text\">get_words()</code> function, that takes in the file path of a plain-text <code class=\"language-text\">.txt</code> as a string. The function should return a list of lowercase words (as strings), in the order that they appear in the text file.</p>\n<p><em>Hint:</em> make use of the <code class=\"language-text\">split()</code> function and <code class=\"language-text\">lower()</code>.</p>\n<h3>Section 2: Transforming Data</h3>\n<p>Getting data into a useable format is often most important part of the machine learning process. For an n-gram model, this means taking our list of words and creating our list of n-grams, provided the value for <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span>.</p>\n<p><strong>TODO:</strong> Implement <code class=\"language-text\">get_ngrams()</code>, which takes a list of words and the size of the grams and returns a <strong>list of tuples</strong>, where each tuple is a gram.</p>\n<h3>Section 3: Computing the Distribution of Words</h3>\n<p>The most important part of the n-gram model is our estimate of the distribution of our words. That is, we want to map a particular context n-gram to possible next n-grams and their frequencies. We'll represent this distribution by using a double dictionary, where the key of the outer dictionary is the context n-gram, and the inner dictionary maps the target n-gram to its frequency.</p>\n<p>For example, many our <code class=\"language-text\">counts</code> dictionary looks something like:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">counts <span class=\"token operator\">=</span> get_counts<span class=\"token punctuation\">(</span>n_grams<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>counts<span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token string\">'I'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'am'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">(</span><span class=\"token string\">'am'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'cool'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token number\">50</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'am'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'lame'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'am'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'asleep'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span> <span class=\"token number\">10</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span> <span class=\"token punctuation\">}</span></code></pre></div>\n<p>We also want to make sure that our model <em>generalizes</em> a bit better than just the raw frequencies. So we'll also add 1 to each possible frequency so that our model has a bit more creativity. If you're interested, this is called <a href=\"https://en.wikipedia.org/wiki/Language_model#n-gram\">smoothing</a>.</p>\n<p><em>Hint:</em> We can do this by initializing a default dictionary, which maps to a default dictionary with the default integer value of 1: </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">counts <span class=\"token operator\">=</span> defaultdict<span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span><span class=\"token punctuation\">:</span> defaultdict<span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>TODO:</strong> Implement the <code class=\"language-text\">get_counts()</code> function, which takes in the list of n-grams and returns the frequency distribution of the n-grams.</p>\n<p><em>Hint:</em> The <code class=\"language-text\">defaultdict</code> with a bunch of <code class=\"language-text\">lambda</code> functions may be hard to interpret when <code class=\"language-text\">print</code>-ing. I would recommended casting the <code class=\"language-text\">defaultdict</code> to a regular <code class=\"language-text\">dict</code> to make this cleaner and easier to read.</p>\n<h3>Section 4: Generating Words By Sampling</h3>\n<p>Now, it's time for the moment of truth: using our distribution of frequencies to select the next word given some context. </p>\n<p>One way to randomly select a next word would be to select a random number between 1 and <code class=\"language-text\">len(n_grams)</code>, and select that n-gram. However, that totally disregards our actual empirical estimate of the frequencies (i.e. in <code class=\"language-text\">get_counts</code>). A better way would be to randomly select a word in accordance with the distribution we computed earlier.</p>\n<p>We can do this with <a href=\"https://numpy.org/\">NumPy</a> by using <code class=\"language-text\">np.random.choice</code>, where we provide the length of a list to sample from and a list of corresponding <strong>probabilities</strong>, which will return the index of the selection:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">word <span class=\"token operator\">=</span> words<span class=\"token punctuation\">[</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>choice<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> p<span class=\"token operator\">=</span>probabilities<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p><strong>TODO:</strong> Implement the <code class=\"language-text\">generate_gram()</code> function, which takes in the distribution <code class=\"language-text\">counts</code> as well as the context n-gram, and returns a selected n-gram tuple according to the distribution for the given context.</p>\n<p><em>Hint:</em> Make sure you understand which dictionary in <code class=\"language-text\">counts</code> to use as the distribution. You also will need to normalize the raw frequencies into probabilities for <code class=\"language-text\">np.random.choice</code> by dividing each entry by the sum of the frequencies in the distribution.</p>\n<p>There's a lot more nuance to generating text than just random sampling, but these suffices for now. If you want to learn more about how to generate text, check out <a href=\"https://arun.ai/blog/decoding\">this blog post I wrote</a> on the subject.</p>\n<h3>Section 5: Generating Entire Sentences</h3>\n<p>For the final part, we will put everything together. In order to generate an entire sentence or paragraph, we can repeatedly call <code class=\"language-text\">generate_gram()</code>, feeding in the previous prediction as the next context. This is known as <strong>auto-regressive</strong> generation.</p>\n<p><strong>TODO:</strong> Implement the <code class=\"language-text\">generate_sentence</code> function, which takes in the distribution <code class=\"language-text\">counts</code> as well as a context n-gram (and an optional parameter for the number of n-grams to generate) and should return a list of tuples.</p>\n<p><em>Hint:</em> We've provided a <code class=\"language-text\">stringify()</code> helper function to help you visualize what your generated sentence looks like in regular string format. Feel free to use it to help appreciate your work!</p>\n<h3>Section 6: Build Your Own Corpus</h3>\n<p>The corpus provided consists of presidential speeches from Barack Obama, which can be a little dry. In this section, create your own corpus of text, and be creative with it! It can be things like Wikipedia, blog posts, tweets, fan-fiction, or your friend's literary assignments!</p>\n<p>Include the corpus along with a sample generation in your Gradescope submission.</p>\n<h2>Conclusion</h2>\n<p>Machine learning is a very interesting, but often confusing subject. In this course, unfortunately, we can't provide all the details and depth, but this assignment should serve as a springboard for all sorts of things related to data science. The final project will be a great opportunity to explore some of these ideas in greater depth.</p>\n<p>In the next few assignments and lectures, we'll be introducing modern approaches to this very problem of text generation, such as word embeddings and neural networks!</p>"}},"pageContext":{"pathSlug":"/assignments/2"}},"staticQueryHashes":[]}